{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Continuous Pareto Exploration in Multi-Task Learning\n",
    "Source code for ICML submission #640 \"Efficient Continuous Pareto Exploration in Multi-Task Learning\"\n",
    "\n",
    "This script generates Figure 4 in the paper.\n",
    "\n",
    "# Problem setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import pickle\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "from scipy.sparse.linalg import LinearOperator, minres\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from common import *\n",
    "\n",
    "# Fix the random seed.\n",
    "np.random.seed(42)\n",
    "# Use _ to slient manual_seed.\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading MNIST\n",
    "Here we create 2048 images by overlapping images from MNIST with random perturbations. We start from downloading the original MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST training images and labels into root_folder.\n",
    "# Returns (images, labels).\n",
    "# images is of dimension 60k x h x w, labels is of dimension 60k.\n",
    "def download_mnist_training(root_folder):\n",
    "    # Helper function.\n",
    "    def get_int(b):\n",
    "        return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "    # Download data.\n",
    "    image_url = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "    label_url = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "    for url in (image_url, label_url):\n",
    "        data = urllib.request.urlopen(url)\n",
    "        name = url.rpartition('/')[2]\n",
    "        file_path = os.path.join(root_folder, name)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(data.read())\n",
    "        with open(file_path.replace('.gz', ''), 'wb') as out_f, gzip.GzipFile(file_path) as zip_f:\n",
    "            out_f.write(zip_f.read())\n",
    "        os.remove(file_path)\n",
    "\n",
    "    # Extract images.\n",
    "    with open(os.path.join(root_folder, 'train-images-idx3-ubyte'), 'rb') as f:\n",
    "        data = f.read()\n",
    "        # Check the magic number and metadata.\n",
    "        assert get_int(data[:4]) == 2051\n",
    "        length = get_int(data[4:8])\n",
    "        assert length == 60000\n",
    "        num_rows = get_int(data[8:12])\n",
    "        num_cols = get_int(data[12:16])\n",
    "        assert num_rows == num_cols == 28\n",
    "        # Read images.\n",
    "        image_data = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "        images = image_data.reshape(length, num_rows, num_cols)\n",
    "\n",
    "    # Extract labels.\n",
    "    with open(os.path.join(root_folder, 'train-labels-idx1-ubyte'), 'rb') as f:\n",
    "        data = f.read()\n",
    "        # Check the magic number and metadata.\n",
    "        assert get_int(data[:4]) == 2049\n",
    "        length = get_int(data[4:8])\n",
    "        assert length == 60000\n",
    "        label_data = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "        labels = label_data.ravel()\n",
    "    return images, labels\n",
    "\n",
    "root_folder = 'MultiMNISTSubset'\n",
    "mnist_images, mnist_labels = download_mnist_training(root_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sanity check, we display the labels and images below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MNIST.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "choice = np.random.choice(mnist_labels.size, 16, replace=False)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax = fig.add_subplot(4, 4, i * 4 + j + 1)\n",
    "        ax.matshow(mnist_images[choice[i * 4 + j]])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('label: {}'.format(mnist_labels[choice[i * 4 + j]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating MultiMNIST\n",
    "\n",
    "Now we generate MultiMNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns (images, left_label, right_label).\n",
    "# images is of size (number, h, w), left_label and right_label are of size (number,)\n",
    "def generate_multi_mnist(number):\n",
    "    multi_images = []\n",
    "    left_labels = []\n",
    "    right_labels = []\n",
    "    mnist_size = mnist_labels.size\n",
    "    left = np.random.permutation(mnist_size)[:number]\n",
    "    right = np.random.permutation(mnist_size)[:number]\n",
    "    for l, r in zip(left, right):\n",
    "        left_labels.append(mnist_labels[l])\n",
    "        right_labels.append(mnist_labels[r])\n",
    "\n",
    "        left_image = mnist_images[l]\n",
    "        right_image = mnist_images[r]\n",
    "        # Randomly shift left and right images.\n",
    "        if np.random.rand() < 0.5:\n",
    "            shift_left = np.random.randint(3)\n",
    "            shift_right = np.random.randint(4)\n",
    "        else:\n",
    "            shift_left = np.random.randint(4)\n",
    "            shift_right = np.random.randint(3)\n",
    "        \n",
    "        new_image = np.zeros((36, 36))\n",
    "        new_image[shift_left:shift_left + 28, shift_left:shift_left + 28] += left_image\n",
    "        new_image[8 - shift_right:36 - shift_right, 8 - shift_right:36 - shift_right] += right_image\n",
    "\n",
    "        # Post-processing.\n",
    "        new_image = np.clip(new_image, 0, 255).astype(mnist_images[0].dtype)\n",
    "        # Downsample the image to 14 x 14.\n",
    "        new_image = np.array(Image.fromarray(new_image).resize((14, 14), resample=Image.NEAREST))\n",
    "        multi_images.append(new_image)\n",
    "    return multi_images, left_labels, right_labels\n",
    "\n",
    "subset_size = 2048\n",
    "multi_images, left_labels, right_labels = generate_multi_mnist(subset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as a sanity check, we visualize the labels and digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MultiMNIST.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "choice = np.random.choice(subset_size, 16, replace=False)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax = fig.add_subplot(4, 4, i * 4 + j + 1)\n",
    "        idx = choice[i * 4 + j]\n",
    "        ax.matshow(multi_images[idx])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('label: {}, {}'.format(left_labels[idx], right_labels[idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our method\n",
    "\n",
    "### Network and task definition\n",
    "We start with the definition of our neural network and loss functions. This network has two tails, one for each classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network.\n",
    "class MiniLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MiniLeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, (5, 5), stride=2)\n",
    "        self.fc1 = nn.Linear(40, 20)\n",
    "        self.fc3_1 = nn.Linear(20, 10)\n",
    "        self.fc3_2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = [self.fc3_1(x), self.fc3_2(x)]\n",
    "        return x\n",
    "\n",
    "network = MiniLeNet()\n",
    "x0 = ndarray(parameters_to_vector(network.parameters()).clone().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss.\n",
    "# Transform images to torch tensors and normalize them.\n",
    "# For MNIST, mean = 0.1307, std = 0.3081\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# Dim of multi_images_torch: #images x #channels x height x width.\n",
    "multi_images_torch = torch.stack([transform(Image.fromarray(img)) for img in multi_images], dim=0)\n",
    "\n",
    "# Label shape: (#images,)\n",
    "left_labels_torch = torch.from_numpy(ndarray(left_labels)).long()\n",
    "right_labels_torch = torch.from_numpy(ndarray(right_labels)).long()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def flatten_node(node):\n",
    "    for i, (node_module, param) in enumerate(zip(node, network.parameters())):\n",
    "        if node_module is None:\n",
    "            node[i] = torch.zeros_like(param)\n",
    "    node_vec = parameters_to_vector(node)\n",
    "    return node_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Pareto front\n",
    "\n",
    "We then generate empirical Pareto front by minimizing the weighted sum of losses with varying weights. Depending on your choice of `num_trials`, this process can take a while. For 101 trials, this cell took around 100 minutes to finish on our computer with PyTorch CPU. We actually used GPU in our experiments but we want to keep this script simple to set up.\n",
    "\n",
    "We have also attached the training results in this code repository for num_trials = 101. So if you don't plan to use a new `num_trials`, executing this scripyt will load these files directly and will be super fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 101\n",
    "for w1 in np.linspace(0, 1, num_trials):\n",
    "    w2 = 1 - w1\n",
    "\n",
    "    def loss_and_grad(x):\n",
    "        x_cp = np.copy(x)\n",
    "        # Convert x to tensor.\n",
    "        x_torch = torch.as_tensor(x_cp, dtype=torch.float)\n",
    "\n",
    "        # Compute loss.\n",
    "        vector_to_parameters(x_torch, network.parameters())\n",
    "        logits = network(multi_images_torch)\n",
    "        loss_left, loss_right = loss_function(logits[0], left_labels_torch), loss_function(logits[1], right_labels_torch)\n",
    "        loss_torch = w1 * loss_left + w2 * loss_right\n",
    "        loss = loss_torch.double().clone().detach().cpu().numpy()\n",
    "\n",
    "        # Compute gradients.\n",
    "        grad = 0\n",
    "        for loss_node, w in [(loss_left, w1), (loss_right, w2)]:\n",
    "            grad_node = list(torch.autograd.grad(loss_node, network.parameters(), retain_graph=True, allow_unused=True))\n",
    "            grad_vec = flatten_node(grad_node)\n",
    "            grad += grad_vec.double().clone().detach().numpy() * w\n",
    "        return loss, grad\n",
    "\n",
    "    data_file = os.path.join(root_folder, '{:.2f}_{:.2f}.bin'.format(w1, w2))\n",
    "    if not os.path.exists(data_file):\n",
    "        result = scipy.optimize.minimize(loss_and_grad, x0, method='L-BFGS-B', jac=True, bounds=None)\n",
    "        x = result.x\n",
    "        pickle.dump(x, open(data_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell generated 101 solutions but not all of them are Pareto optimal. We now post-process these solutions by filtering out dominated ones and those with extremely bad losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out dominated solutions.\n",
    "\n",
    "# Helper function.\n",
    "# Input: numpy array of size 1500.\n",
    "# Output: two scalars representing the left loss and right loss.\n",
    "def get_loss(x):\n",
    "    x_cp = np.copy(x)\n",
    "    x_torch = torch.as_tensor(x_cp, dtype=torch.float)\n",
    "    vector_to_parameters(x_torch, network.parameters())\n",
    "    logits = network(multi_images_torch)\n",
    "    loss_left, loss_right = loss_function(logits[0], left_labels_torch), loss_function(logits[1], right_labels_torch)\n",
    "    loss_left = loss_left.double().clone().detach().cpu().numpy()\n",
    "    loss_right = loss_right.double().clone().detach().cpu().numpy()\n",
    "    return loss_left, loss_right\n",
    "\n",
    "pareto = []\n",
    "for w1 in np.linspace(0, 1, num_trials):\n",
    "    w2 = 1 - w1\n",
    "    data_file = os.path.join(root_folder, '{:.2f}_{:.2f}.bin'.format(w1, w2))\n",
    "    x = pickle.load(open(data_file, 'rb'))\n",
    "    losses = ndarray(get_loss(x))\n",
    "    pareto.append((np.copy(x), np.copy(losses), w1, w2))\n",
    "\n",
    "# Filter out extremely bad solutions.\n",
    "def in_range(f):\n",
    "    return 0 <= f[0] <= 2.0 and 0.5 <= f[1] <= 2.5\n",
    "\n",
    "pareto_filtered = []\n",
    "for x, losses, w1, w2 in pareto:\n",
    "    # Filter out dominated solutions.\n",
    "    if np.any([np.min(losses - losses2) > 0 for _, losses2, _, _ in pareto]) or not in_range(losses): continue\n",
    "    pareto_filtered.append((np.copy(x), np.copy(losses), w1, w2))\n",
    "\n",
    "# Sort pareto_filtered.\n",
    "pareto_filtered = sorted(pareto_filtered, key=lambda x: x[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the empirical Pareto front:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the empirical Pareto front.\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "pareto_front = ndarray([np.copy(l) for _, l, _, _ in pareto_filtered])\n",
    "pareto_front = ndarray(pareto_front)\n",
    "ax.plot(pareto_front[:, 0], pareto_front[:, 1], 'k-.')\n",
    "ax.scatter(pareto_front[:, 0], pareto_front[:, 1], c='tab:red')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('$f_1$')\n",
    "ax.set_ylabel('$f_2$')\n",
    "ax.set_xticks(np.linspace(0, 2.0, 5))\n",
    "ax.set_yticks(np.linspace(0.5, 2.5, 5))\n",
    "ax.set_xlim([-0.25, 2.0])\n",
    "ax.set_ylim([0.5, 2.5])\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define our method and the weighted sum baseline for Pareto expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two expansion strategies.\n",
    "\n",
    "# A helper function for gradient correction.\n",
    "def correct_gradients(g, alpha):\n",
    "    g_corrected = ndarray([gi - alpha.T @ g for gi in g])\n",
    "    # We can still use the same alpha.\n",
    "    return g_corrected\n",
    "\n",
    "# Given network parameters x (a numpy array), return a 2 x n gradient numpy array.\n",
    "def get_grad(x):\n",
    "    x_cp = np.copy(x)\n",
    "    # Convert x to tensor.\n",
    "    x_torch = torch.as_tensor(x_cp, dtype=torch.float)\n",
    "    vector_to_parameters(x_torch, network.parameters())\n",
    "    logits = network(multi_images_torch)\n",
    "    loss_left, loss_right = loss_function(logits[0], left_labels_torch), loss_function(logits[1], right_labels_torch)\n",
    "\n",
    "    # Compute gradients.\n",
    "    grad_np = []\n",
    "    for loss_node in (loss_left, loss_right):\n",
    "        grad_node = list(torch.autograd.grad(loss_node, network.parameters(), retain_graph=True, allow_unused=True))\n",
    "        grad_vec = flatten_node(grad_node)\n",
    "        grad_np.append(grad_vec.double().clone().detach().numpy())\n",
    "    return ndarray(grad_np)\n",
    "\n",
    "# The WeightedSum baseline at one single Pareto optimal solution: given a weight vector, run GD for num_iter iterations\n",
    "# to minimize w[0] * f[0] + w[1] * f[1]\n",
    "# Input:\n",
    "# - x_init: a 1500-dimensional numpy array representing the neural network parameter;\n",
    "# - num_iter: the maximum allowable number of gradient-descend iterations;\n",
    "# - w: the weight vector (2 dimensional).\n",
    "# Output:\n",
    "# - A list of intermediate solutions (x[i], f[i]) explored by GD at each iteration.\n",
    "def expand_ws(x_init, num_iter, w):\n",
    "    # Hyperparameters for WeightedSum.\n",
    "    lr_init = 5e-2\n",
    "    history = [(np.copy(x_init), ndarray(get_loss(x_init)))]\n",
    "    for t in range(num_iter):\n",
    "        lr = lr_init / np.sqrt(t + 1)\n",
    "        # Compute the gradient.\n",
    "        xi = np.copy(history[-1][0])\n",
    "        g = ndarray(w).T @ get_grad(xi)\n",
    "        xi -= lr * g\n",
    "        history.append((xi, ndarray(get_loss(xi))))\n",
    "    return history\n",
    "\n",
    "# Similar to expand_ws except that bfgs is used instead of gd.\n",
    "def expand_bfgs(x_init, num_iter, w):\n",
    "    history = []\n",
    "    def loss_and_grad(x):\n",
    "        loss = get_loss(x)\n",
    "        grad = get_grad(x)\n",
    "        loss, grad = ndarray(w).dot(ndarray(loss)), ndarray(w).T @ grad\n",
    "        history.append((np.copy(x), ndarray(get_loss(x))))\n",
    "        return loss, grad\n",
    "\n",
    "    result = scipy.optimize.minimize(loss_and_grad, x_init, method='L-BFGS-B', jac=True, bounds=None, options={ 'maxiter': num_iter })\n",
    "    x = result.x\n",
    "    return history\n",
    "\n",
    "# The MINRES method at one single Pareto optimal solution: run MINRES for num_iter iterations then move x0 along the\n",
    "# approximated tangent.\n",
    "# Input:\n",
    "# - x_init: a 1500-dimensional numpy array representing the neural network parameter;\n",
    "# - num_iter: the maximum allowable number of MINRES iterations;\n",
    "# Output:\n",
    "# - A 1500-dimensional tangent direction.\n",
    "def expand_minres(x_init, num_iter):\n",
    "    # Solve alpha in MGDA: note that this does not increment the counter and g[0] and g[1] are not necessarily parallel.\n",
    "    def compute_alpha(g):\n",
    "        g = ndarray(g)\n",
    "        alpha = cp.Variable(2)\n",
    "        objective = cp.Minimize(cp.sum_squares(alpha.T @ g))\n",
    "        constraints = [alpha >= 0, cp.sum(alpha) == 1]\n",
    "        alpha_prob = cp.Problem(objective, constraints)\n",
    "        optimal_loss = alpha_prob.solve()\n",
    "        alpha = ndarray(alpha.value).ravel()\n",
    "        return alpha\n",
    "    grad = get_grad(x_init)\n",
    "    alpha = compute_alpha(grad)\n",
    "    grad = correct_gradients(grad, alpha)\n",
    "    assert np.allclose(alpha.T @ grad, 0)\n",
    "    n = grad.shape[1]\n",
    "\n",
    "    # Matrix-free iterative solver.\n",
    "    x_torch = torch.as_tensor(x_init, dtype=torch.float)\n",
    "    vector_to_parameters(x_torch, network.parameters())\n",
    "    logits = network(multi_images_torch)\n",
    "    loss_left, loss_right = loss_function(logits[0], left_labels_torch), loss_function(logits[1], right_labels_torch)\n",
    "    # Compute the grad node.\n",
    "    grad_left = list(torch.autograd.grad(loss_left, network.parameters(), retain_graph=True, allow_unused=True, create_graph=True))\n",
    "    grad_right = list(torch.autograd.grad(loss_right, network.parameters(), retain_graph=True, allow_unused=True, create_graph=True))\n",
    "    grad_left_vec = flatten_node(grad_left)\n",
    "    grad_right_vec = flatten_node(grad_right)\n",
    "    weighted_sum_grad = alpha[0] * grad_left_vec + alpha[1] * grad_right_vec\n",
    "    def H_op(y):\n",
    "        y = ndarray(y).ravel()\n",
    "        assert y.size == n\n",
    "        dot_prod = weighted_sum_grad.mul(torch.as_tensor(y, dtype=torch.float)).sum()\n",
    "        hvp = list(torch.autograd.grad(dot_prod, network.parameters(), retain_graph=True, allow_unused=True))\n",
    "        hvp = flatten_node(hvp)\n",
    "        return ndarray(hvp.clone().detach().numpy())\n",
    "\n",
    "    # Randomly pick a vector from the column space of grad.T\n",
    "    rhs = np.random.normal(size=2).T @ grad\n",
    "    rhs /= np.linalg.norm(rhs)\n",
    "    result = minres(LinearOperator((n, n), matvec=H_op, rmatvec=H_op), rhs, maxiter=num_iter)\n",
    "    v = result[0]\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "def expand_random(x_init, num_iter):\n",
    "    return np.random.normal(size=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "We now conduct the experiments and visualize the results. Note that similar to what has happened in ZDT2-variant, weighted sum (blue and green) quickly generates directions that lead to the dominated interior while moving along the approximated tangent directions tracks the emprical Pareto front better.\n",
    "\n",
    "This experiment takes a few minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Plot the WeightedSum baseline -- you can try adding new weights or replace expand_ws with expand_bfgs too.\n",
    "# Each iter at expand_ws consumes 1 call to eval f and 1 call to eval grad.\n",
    "num_iter = 50\n",
    "\n",
    "for w, color, label in [([1, 0], 'tab:blue', '$GD-f_1$'), ([0, 1], 'tab:green', '$GD-f_2$')]:\n",
    "    for i, (xi, _, _, _) in enumerate(pareto_filtered):\n",
    "        sys.stdout.write('\\r{} progress: {:5.2f}% done...'.format(label.replace('$', ''), (i + 1) / len(pareto_filtered) * 100))\n",
    "        sys.stdout.flush()\n",
    "        xi = pareto_filtered[i][0]\n",
    "        history = expand_ws(xi, num_iter, w)\n",
    "        traj = ndarray([fi for _, fi in history])\n",
    "        ax.plot(traj[:, 0], traj[:, 1], c=color, alpha=0.2, label=label if i == 0 else None)\n",
    "    sys.stdout.write('\\n')\n",
    "\n",
    "# Plot the MINRES results -- you can replace expand_minres with expand_random if you want to know if tangent directions\n",
    "# are truly effective.\n",
    "# Each iter at expand_minres consumes 1 call to eval hvp.\n",
    "for i, (xi, _, _, _) in enumerate(pareto_filtered):\n",
    "    sys.stdout.write('\\rMINRES progress: {:5.2f}% done...'.format((i + 1) / len(pareto_filtered) * 100))\n",
    "    sys.stdout.flush()\n",
    "    v = expand_minres(xi, num_iter)\n",
    "    v /= np.linalg.norm(v)\n",
    "    traj = ndarray([get_loss(xi + s * v) for s in np.linspace(-0.5, 0.5, 201)])\n",
    "    ax.plot(traj[:, 0], traj[:, 1], c='tab:red', label='MINRES' if i == 0 else None)\n",
    "sys.stdout.write('\\n')\n",
    "\n",
    "# Plot the Pareto front.\n",
    "ax.plot(pareto_front[:, 0], pareto_front[:, 1], 'k-.', label='Pareto front')\n",
    "ax.scatter(pareto_front[:, 0], pareto_front[:, 1], c='tab:red', s=10, alpha=0.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('$f_1$')\n",
    "ax.set_ylabel('$f_2$')\n",
    "ax.set_xticks(np.linspace(0, 2.0, 5))\n",
    "ax.set_yticks(np.linspace(0.5, 2.5, 5))\n",
    "ax.set_xlim([-0.25, 2.0])\n",
    "ax.set_ylim([0.5, 2.5])\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
