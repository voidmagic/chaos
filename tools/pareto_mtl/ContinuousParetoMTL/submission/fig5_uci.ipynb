{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Continuous Pareto Exploration in Multi-Task Learning\n",
    "\n",
    "Source code for ICML submission #640 \"Efficient Continuous Pareto Exploration in Multi-Task Learning\"\n",
    "\n",
    "This script generates Figure 5 in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import codecs\n",
    "import gzip\n",
    "import os\n",
    "import urllib\n",
    "import pickle\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from contextlib import contextmanager\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "from scipy.sparse.linalg import LinearOperator, minres\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from common import *\n",
    "from min_norm_solver import find_min_norm_element\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random seed fixation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uci_info = '''\n",
    "age: label.\n",
    "class of worker: Not in universe, Federal government, Local government, Never worked, Private, Self-employed-incorporated, Self-employed-not incorporated, State government, Without pay.\n",
    "detailed industry recode: 0, 40, 44, 2, 43, 47, 48, 1, 11, 19, 24, 25, 32, 33, 34, 35, 36, 37, 38, 39, 4, 42, 45, 5, 15, 16, 22, 29, 31, 50, 14, 17, 18, 28, 3, 30, 41, 46, 51, 12, 13, 21, 23, 26, 6, 7, 9, 49, 27, 8, 10, 20.\n",
    "detailed occupation recode: 0, 12, 31, 44, 19, 32, 10, 23, 26, 28, 29, 42, 40, 34, 14, 36, 38, 2, 20, 25, 37, 41, 27, 24, 30, 43, 33, 16, 45, 17, 35, 22, 18, 39, 3, 15, 13, 46, 8, 21, 9, 4, 6, 5, 1, 11, 7.\n",
    "education: label.\n",
    "wage per hour: continuous.\n",
    "enroll in edu inst last wk: Not in universe, High school, College or university.\n",
    "marital stat: label.\n",
    "major industry code: Not in universe or children, Entertainment, Social services, Agriculture, Education, Public administration, Manufacturing-durable goods, Manufacturing-nondurable goods, Wholesale trade, Retail trade, Finance insurance and real estate, Private household services, Business and repair services, Personal services except private HH, Construction, Medical except hospital, Other professional services, Transportation, Utilities and sanitary services, Mining, Communications, Hospital services, Forestry and fisheries, Armed Forces.\n",
    "major occupation code: Not in universe, Professional specialty, Other service, Farming forestry and fishing, Sales, Adm support including clerical, Protective services, Handlers equip cleaners etc , Precision production craft & repair, Technicians and related support, Machine operators assmblrs & inspctrs, Transportation and material moving, Executive admin and managerial, Private household services, Armed Forces.\n",
    "race: White, Black, Other, Amer Indian Aleut or Eskimo, Asian or Pacific Islander.\n",
    "hispanic origin: Mexican (Mexicano), Mexican-American, Puerto Rican, Central or South American, All other, Other Spanish, Chicano, Cuban, Do not know, NA.\n",
    "sex: Female, Male.\n",
    "member of a labor union: Not in universe, No, Yes.\n",
    "reason for unemployment: Not in universe, Re-entrant, Job loser - on layoff, New entrant, Job leaver, Other job loser.\n",
    "full or part time employment stat: Children or Armed Forces, Full-time schedules, Unemployed part- time, Not in labor force, Unemployed full-time, PT for non-econ reasons usually FT, PT for econ reasons usually PT, PT for econ reasons usually FT.\n",
    "capital gains: continuous.\n",
    "capital losses: continuous.\n",
    "dividends from stocks: continuous.\n",
    "tax filer stat: Nonfiler, Joint one under 65 & one 65+, Joint both under 65, Single, Head of household, Joint both 65+.\n",
    "region of previous residence: Not in universe, South, Northeast, West, Midwest, Abroad.\n",
    "state of previous residence: Not in universe, Utah, Michigan, North Carolina, North Dakota, Virginia, Vermont, Wyoming, West Virginia, Pennsylvania, Abroad, Oregon, California, Iowa, Florida, Arkansas, Texas, South Carolina, Arizona, Indiana, Tennessee, Maine, Alaska, Ohio, Montana, Nebraska, Mississippi, District of Columbia, Minnesota, Illinois, Kentucky, Delaware, Colorado, Maryland, Wisconsin, New Hampshire, Nevada, New York, Georgia, Oklahoma, New Mexico, South Dakota, Missouri, Kansas, Connecticut, Louisiana, Alabama, Massachusetts, Idaho, New Jersey.\n",
    "detailed household and family stat: Child <18 never marr not in subfamily, Other Rel <18 never marr child of subfamily RP, Other Rel <18 never marr not in subfamily, Grandchild <18 never marr child of subfamily RP, Grandchild <18 never marr not in subfamily, Secondary individual, In group quarters, Child under 18 of RP of unrel subfamily, RP of unrelated subfamily, Spouse of householder, Householder, Other Rel <18 never married RP of subfamily, Grandchild <18 never marr RP of subfamily, Child <18 never marr RP of subfamily, Child <18 ever marr not in subfamily, Other Rel <18 ever marr RP of subfamily, Child <18 ever marr RP of subfamily, Nonfamily householder, Child <18 spouse of subfamily RP, Other Rel <18 spouse of subfamily RP, Other Rel <18 ever marr not in subfamily, Grandchild <18 ever marr not in subfamily, Child 18+ never marr Not in a subfamily, Grandchild 18+ never marr not in subfamily, Child 18+ ever marr RP of subfamily, Other Rel 18+ never marr not in subfamily, Child 18+ never marr RP of subfamily, Other Rel 18+ ever marr RP of subfamily, Other Rel 18+ never marr RP of subfamily, Other Rel 18+ spouse of subfamily RP, Other Rel 18+ ever marr not in subfamily, Child 18+ ever marr Not in a subfamily, Grandchild 18+ ever marr not in subfamily, Child 18+ spouse of subfamily RP, Spouse of RP of unrelated subfamily, Grandchild 18+ ever marr RP of subfamily, Grandchild 18+ never marr RP of subfamily, Grandchild 18+ spouse of subfamily RP.\n",
    "detailed household summary in household: Child under 18 never married, Other relative of householder, Nonrelative of householder, Spouse of householder, Householder, Child under 18 ever married, Group Quarters- Secondary individual, Child 18 or older.\n",
    "instance weight: ignore.\n",
    "migration code-change in msa: Not in universe, Nonmover, MSA to MSA, NonMSA to nonMSA, MSA to nonMSA, NonMSA to MSA, Abroad to MSA, Not identifiable, Abroad to nonMSA.\n",
    "migration code-change in reg: Not in universe, Nonmover, Same county, Different county same state, Different state same division, Abroad, Different region, Different division same region.\n",
    "migration code-move within reg: Not in universe, Nonmover, Same county, Different county same state, Different state in West, Abroad, Different state in Midwest, Different state in South, Different state in Northeast.\n",
    "live in this house 1 year ago: Not in universe under 1 year old, Yes, No.\n",
    "migration prev res in sunbelt: Not in universe, Yes, No.\n",
    "num persons worked for employer: continuous.\n",
    "family members under 18: Both parents present, Neither parent present, Mother only present, Father only present, Not in universe.\n",
    "country of birth father: Mexico, United-States, Puerto-Rico, Dominican-Republic, Jamaica, Cuba, Portugal, Nicaragua, Peru, Ecuador, Guatemala, Philippines, Canada, Columbia, El-Salvador, Japan, England, Trinadad&Tobago, Honduras, Germany, Taiwan, Outlying-U S (Guam USVI etc), India, Vietnam, China, Hong Kong, Cambodia, France, Laos, Haiti, South Korea, Iran, Greece, Italy, Poland, Thailand, Yugoslavia, Holand-Netherlands, Ireland, Scotland, Hungary, Panama.\n",
    "country of birth mother: India, Mexico, United-States, Puerto-Rico, Dominican-Republic, England, Honduras, Peru, Guatemala, Columbia, El-Salvador, Philippines, France, Ecuador, Nicaragua, Cuba, Outlying-U S (Guam USVI etc), Jamaica, South Korea, China, Germany, Yugoslavia, Canada, Vietnam, Japan, Cambodia, Ireland, Laos, Haiti, Portugal, Taiwan, Holand-Netherlands, Greece, Italy, Poland, Thailand, Trinadad&Tobago, Hungary, Panama, Hong Kong, Scotland, Iran.\n",
    "country of birth self: United-States, Mexico, Puerto-Rico, Peru, Canada, South Korea, India, Japan, Haiti, El-Salvador, Dominican-Republic, Portugal, Columbia, England, Thailand, Cuba, Laos, Panama, China, Germany, Vietnam, Italy, Honduras, Outlying-U S (Guam USVI etc), Hungary, Philippines, Poland, Ecuador, Iran, Guatemala, Holand-Netherlands, Taiwan, Nicaragua, France, Jamaica, Scotland, Yugoslavia, Hong Kong, Trinadad&Tobago, Greece, Cambodia, Ireland.\n",
    "citizenship: Native- Born in the United States, Foreign born- Not a citizen of U S , Native- Born in Puerto Rico or U S Outlying, Native- Born abroad of American Parent(s), Foreign born- U S citizen by naturalization.\n",
    "own business or self employed: 0, 2, 1.\n",
    "fill inc questionnaire for veteran's admin: Not in universe, Yes, No.\n",
    "veterans benefits: 0, 2, 1.\n",
    "weeks worked in year: continuous.\n",
    "year: 94, 95.\n",
    "income: - 50000, 50000+.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCI(torch.utils.data.Dataset):\n",
    "    urls = [\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.data.gz',\n",
    "        'https://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.test.gz'\n",
    "    ]\n",
    "    raw_folder = 'raw'\n",
    "    processed_folder = 'processed'\n",
    "    training_file = 'training.pth'\n",
    "    test_file = 'test.pth'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if train:\n",
    "            self.data, self.labels = torch.load(\n",
    "                self.root / self.processed_folder /self.training_file)\n",
    "        else:\n",
    "            self.data, self.labels = torch.load(\n",
    "                self.root / self.processed_folder / self.test_file)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.labels[index]\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return (self.root / self.processed_folder / self.training_file).is_file() and \\\n",
    "            (self.root / self.processed_folder / self.test_file).is_file()\n",
    "\n",
    "    def download(self):\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # download files\n",
    "        (self.root / self.raw_folder).mkdir(parents=True, exist_ok=True)\n",
    "        (self.root / self.processed_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for url in self.urls:\n",
    "            print('Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[2]\n",
    "            file_path = self.root / self.raw_folder / filename\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "            with open(self.root / self.raw_folder / '.'.join(filename.split('.')[:-1]), 'wb') as out_f, \\\n",
    "                    gzip.GzipFile(file_path) as zip_f:\n",
    "                out_f.write(zip_f.read())\n",
    "            os.unlink(file_path)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "\n",
    "        name_dict = OrderedDict()\n",
    "        property_list = []\n",
    "        for line in uci_info.split('\\n'):\n",
    "            if not line:\n",
    "                continue\n",
    "            name, values = line.strip()[:-1].split(': ')\n",
    "            name_dict[name] = []\n",
    "            if values in ('ignore', 'label', 'continuous'):\n",
    "                pp = values\n",
    "            else:\n",
    "                pp = 'normal'\n",
    "            property_list.append(pp)\n",
    "\n",
    "        self.uci_preprocess(self.root / self.raw_folder / 'census-income.data', name_dict, property_list)\n",
    "        self.uci_preprocess(self.root / self.raw_folder / 'census-income.test', name_dict, property_list)\n",
    "\n",
    "        for i, (name, values) in enumerate(name_dict.items()):\n",
    "            value_set = list(sorted(list(set(values))))\n",
    "            value_dict = dict()\n",
    "            for j, value in enumerate(value_set):\n",
    "                value_dict[value] = j\n",
    "            name_dict[name] = value_dict\n",
    "\n",
    "        training_set = self.uci_process(self.root / self.raw_folder / 'census-income.data', name_dict, property_list)\n",
    "        test_set = self.uci_process(self.root / self.raw_folder / 'census-income.test', name_dict, property_list)\n",
    "\n",
    "        with open(self.root / self.processed_folder / self.training_file, 'wb') as f:\n",
    "            torch.save(training_set, f)\n",
    "        with open(self.root / self.processed_folder / self.test_file, 'wb') as f:\n",
    "            torch.save(test_set, f)\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(\n",
    "            tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(\n",
    "            tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "    @staticmethod\n",
    "    def uci_preprocess(path, name_dict, property_list):\n",
    "        with open(path, 'r') as f:\n",
    "            raw_data = f.readlines()\n",
    "        for line in raw_data:\n",
    "            if len(line.strip()) == 0:\n",
    "                continue\n",
    "            words = line.strip()[:-1].split(', ')\n",
    "            if len(words) != 42:\n",
    "                continue\n",
    "\n",
    "            # make list\n",
    "            for word, pp, (name, l) in zip(words, property_list, name_dict.items()):\n",
    "                word = word.strip()\n",
    "                if pp == 'continuous':\n",
    "                    word = float(word)\n",
    "                l.append(word)\n",
    "\n",
    "    @staticmethod\n",
    "    def uci_process(path, name_dict, property_list):\n",
    "        with open(path, 'r') as f:\n",
    "            raw_data = f.readlines()\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "        for line in raw_data:\n",
    "            if len(line.strip()) == 0:\n",
    "                continue\n",
    "            words = line.strip()[:-1].split(', ')\n",
    "            if len(words) != 42:\n",
    "                continue\n",
    "\n",
    "            # make list\n",
    "            image = []\n",
    "            label = [None, None, None]\n",
    "            for word, pp, (name, values) in zip(words, property_list, name_dict.items()):\n",
    "                word = word.strip()\n",
    "                if pp == 'continuous':\n",
    "                    word = float(word)\n",
    "                    image.append(word)\n",
    "                elif pp == 'ignore':\n",
    "                    continue\n",
    "                elif pp == 'label':\n",
    "                    if name == 'education':\n",
    "                        label[1] = int(word.startswith(('Bachelors', 'Some', 'Maters', 'Asso', 'Doctorate', 'Prof')))\n",
    "                    elif name == 'marital stat':\n",
    "                        label[2] = int(word == 'Never married')\n",
    "                    else: # age\n",
    "                        label[0] = int(float(word) >= 40)\n",
    "                else:\n",
    "                    # normal\n",
    "                    one_hot = np.zeros(len(values))\n",
    "                    one_hot[values[word]] = 1\n",
    "                    image.append(one_hot)\n",
    "\n",
    "            images.append(torch.Tensor(np.hstack(image)))\n",
    "            labels.append(torch.LongTensor(label))\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UCI(root='./UCI', train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch initialization\n",
    "\n",
    "- working directory\n",
    "- device\n",
    "- dataloader\n",
    "- utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_root = Path('./UCI/checkpoints')\n",
    "ckpt_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sgd_path = ckpt_root / 'sgd'\n",
    "sgd_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mr_path = ckpt_root / 'minres'\n",
    "mr_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Checkpoint root:', ckpt_root)\n",
    "print('SGD path:       ', sgd_path)\n",
    "print('MINRES path:    ', mr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computating device initialization\n",
    "We remove all random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')  # use default cuda device\n",
    "    import torch.backends.cudnn as cudnn  # make cuda deterministic\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "else:\n",
    "    device = torch.device('cpu') # otherwise use cpu\n",
    "\n",
    "print('Current device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test dataloader\n",
    "We use batch size of 256 for both training and test side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = UCI('./UCI', train=True, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "testset = UCI('./UCI', train=False, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "print('Training Dataset:')\n",
    "print(trainset)\n",
    "print()\n",
    "\n",
    "print('Test Dataset:')\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "- evenly distributed weights\n",
    "- top-k accuracies\n",
    "- evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evenly_dist(num_weights, dim=3):\n",
    "    return [ret for ret in product(np.linspace(0.0, 1.0, num_weights + 2), repeat=dim) \\\n",
    "            if round(sum(ret), 6) == 1.0 and all(r not in (0.0, 1.0) for r in ret)]\n",
    "\n",
    "def topk_accuracies(logits, targets, ks=(1,)):\n",
    "    assert logits.dim() == 2\n",
    "    assert targets.dim() == 1\n",
    "    assert logits.size(0) == targets.size(0)\n",
    "\n",
    "    maxk = max(ks)\n",
    "    _, pred = logits.topk(maxk, dim=1, largest=True, sorted=True)\n",
    "    targets = targets.unsqueeze(1).expand_as(pred)\n",
    "    correct = pred.eq(targets).float()\n",
    "\n",
    "    accu_list = []\n",
    "    for k in ks:\n",
    "        accu = correct[:, :k].sum(1).mean()\n",
    "        accu_list.append(accu.item())\n",
    "    return accu_list\n",
    "\n",
    "def evaluate(network, dataloader, closures, topk_closures):\n",
    "    num_samples = 0\n",
    "    total_losses = np.zeros(len(closures))\n",
    "    total_top1s = np.zeros(len(closures))\n",
    "    with torch.no_grad():\n",
    "        network.train(False)\n",
    "        for images, targets in dataloader:\n",
    "            batch_size = len(images)\n",
    "            num_samples += batch_size\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = network(images)\n",
    "            losses = [c(network, logits, targets).item() for c in closures]\n",
    "            total_losses += batch_size * np.array(losses)\n",
    "            topks = [c(network, logits, targets) for c in topk_closures]\n",
    "            total_top1s += batch_size * np.array(topks)\n",
    "    total_losses /= num_samples\n",
    "    total_top1s /= num_samples\n",
    "    return total_losses, total_top1s\n",
    "\n",
    "print('Example of evenly_dist(num_weights=5, dim=3):')\n",
    "for i, combination in enumerate(evenly_dist(5, dim=3)):\n",
    "    print('{:d}: ('.format(i + 1) + ', '.join(['{:.3f}'.format(digit) for digit in combination]) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Pareto front generation\n",
    "\n",
    "- hyper-parameters\n",
    "- network\n",
    "- loss function\n",
    "- optimizer\n",
    "- learning rate scheduler\n",
    "- inital state snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters declaration\n",
    "- num of epochs\n",
    "- num of different weight combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "num_weights = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition\n",
    "\n",
    "We use a double-layer MLP with a fully-connected layer for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(487, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc_age = nn.Linear(128, 2)\n",
    "        self.fc_education = nn.Linear(128, 2)\n",
    "        self.fc_marriage = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        return [self.fc_age(x), self.fc_education(x), self.fc_marriage(x)]\n",
    "\n",
    "network = MLP()\n",
    "network.to(device)\n",
    "\n",
    "print('Network:')\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function definition\n",
    "We use cross entropy loss for two tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "closures = [\n",
    "    lambda n, l, t: criterion(l[0], t[:, 0]),\n",
    "    lambda n, l, t: criterion(l[1], t[:, 1]),\n",
    "    lambda n, l, t: criterion(l[2], t[:, 2])\n",
    "]\n",
    "\n",
    "top1_closures = [\n",
    "    lambda n, l, t: topk_accuracies(l[0], t[:, 0], ks=(1,))[0],\n",
    "    lambda n, l, t: topk_accuracies(l[1], t[:, 1], ks=(1,))[0],\n",
    "    lambda n, l, t: topk_accuracies(l[2], t[:, 2], ks=(1,))[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer definition\n",
    "We use SGD with learning rate of 0.001 and momentum of 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(network.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduler definition\n",
    "We use cosine annealing learning rate scheduler for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = CosineAnnealingLR(optimizer, num_epochs * len(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot for inital states\n",
    "\n",
    "The initial weights/optimizer/lr_scheduler are saved for further training (we removed **ALL** randomness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ckpt = {\n",
    "    'state_dict': network.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr_scheduler': lr_scheduler.state_dict()\n",
    "}\n",
    "torch.save(init_ckpt, sgd_path / 'init.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, weight in enumerate(tqdm(evenly_dist(num_weights, 3), desc='Weight', leave=False)):\n",
    "    init_ckpt = torch.load(sgd_path / 'init.pth', map_location='cpu')  # load init snapshot\n",
    "    network.load_state_dict(init_ckpt['state_dict'])\n",
    "    optimizer.load_state_dict(init_ckpt['optimizer'])\n",
    "    lr_scheduler.load_state_dict(init_ckpt['lr_scheduler'])\n",
    "    with trange(num_epochs, desc='Epoch') as epoch_iter:\n",
    "        for epoch in epoch_iter:\n",
    "            network.train(True)\n",
    "            for images, targets in tqdm(trainloader, desc='Batch', leave=False):\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                logits = network(images)\n",
    "                losses = [c(network, logits, targets) for c in closures]\n",
    "                loss = sum(w * l for w, l in zip(weight, losses))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "            eval_losses, eval_top1s = evaluate(network, testloader, closures, top1_closures)\n",
    "            epoch_iter.set_postfix(**{'acc-{:d}'.format(i + 1): top for i, top in enumerate(eval_top1s)})\n",
    "    ckpt = {\n",
    "        'state_dict': network.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'lr_scheduler': lr_scheduler.state_dict(),\n",
    "        'metrics': [eval_losses, eval_top1s]\n",
    "    }\n",
    "    torch.save(ckpt, sgd_path / '{:d}.pth'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD results illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw=dict(projection='3d', proj_type='ortho'))\n",
    "total_top1s = []\n",
    "for i, weight in enumerate(evenly_dist(num_weights, 3)):\n",
    "    ckpt = torch.load(sgd_path / '{:d}.pth'.format(i), map_location='cpu')\n",
    "    losses, top1s = ckpt['metrics']\n",
    "    total_top1s.append(top1s)\n",
    "total_err1s = 100.0 * (1.0 - np.stack(total_top1s, axis=0).T)\n",
    "ax.scatter(*total_err1s, color='tab:red', marker='*', s=200, label='SGD')\n",
    "ax.set_xlabel('Task 1 Top-1 Error')\n",
    "ax.set_ylabel('Task 2 Top-1 Error')\n",
    "ax.set_zlabel('Task 3 Top-1 Error')\n",
    "ax.legend()\n",
    "ax.view_init(30, 45)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINRES initalization\n",
    "- hyper-parameters\n",
    "- dataloader\n",
    "- optimizer\n",
    "- Jacobian solver\n",
    "- linear operator\n",
    "- utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters declaration\n",
    "- num of steps\n",
    "- damping for linear solver\n",
    "- maxiter for MINRES\n",
    "- momentum for Jacobians and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 20\n",
    "damping = 0.1\n",
    "maxiter = 100\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader definition\n",
    "\n",
    "We explore based on 2048 data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_dataloader = torch.utils.data.DataLoader(trainset, batch_size=2048, shuffle=True, drop_last=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer definition\n",
    "We use SGD with learning rate of 0.01 (**without** momentum for fair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_optimizer = SGD(network.parameters(), lr=0.01)\n",
    "\n",
    "print(mr_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobians solver definition\n",
    "We iterate over trainset to solve jacobian with respect to each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobian_trainiter = iter(trainloader)\n",
    "def compute_jacobians(ratio=1.0):\n",
    "    global jacobian_trainiter\n",
    "    num_batches = int(len(trainloader) * ratio)\n",
    "    jacobians = None\n",
    "    for _ in range(num_batches):\n",
    "        try:\n",
    "            images, targets = next(jacobian_trainiter)\n",
    "        except StopIteration:\n",
    "            jacobian_trainiter = iter(trainloader)\n",
    "            images, targets = next(jacobian_trainiter)\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = network(images)\n",
    "        losses = [c(network, logits, targets) for c in closures]\n",
    "        param_grads = [list(torch.autograd.grad(\n",
    "            l, network.parameters(), allow_unused=True,\n",
    "            retain_graph=True, create_graph=False)) for l in losses]\n",
    "        for param_grad in param_grads:\n",
    "            for i, (param_grad_module, param) in enumerate(zip(param_grad, network.parameters())):\n",
    "                if param_grad_module is None:\n",
    "                    param_grad[i] = torch.zeros_like(param)\n",
    "        sub_jacobians = torch.stack([parameters_to_vector(param_grad) for param_grad in param_grads], dim=0)\n",
    "        sub_jacobians.detach_()\n",
    "        if jacobians is None:\n",
    "            jacobians = sub_jacobians\n",
    "        else:\n",
    "            jacobians.add_(sub_jacobians)\n",
    "    jacobians.div_(num_batches)\n",
    "    return jacobians.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha solver definition\n",
    "We solve alpha by its analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alpha(jacobians):\n",
    "    sol, min_norm = find_min_norm_element(jacobians)\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear operator for Hessian-vector product definition\n",
    "We warp Hessian-vector product into a linear operator to prevent explicit computation of Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HVPLinearOperator(LinearOperator):\n",
    "    def __init__(self, dataloader):\n",
    "        network_size = sum(p.numel() for p in network.parameters())\n",
    "        shape = (network_size, network_size)\n",
    "        dtype = list(network.parameters())[0].detach().cpu().numpy().dtype\n",
    "\n",
    "        super(HVPLinearOperator, self).__init__(dtype, shape)\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.dataiter = iter(dataloader)\n",
    "\n",
    "        self.alpha_jacobians = None\n",
    "\n",
    "    def _get_jacobians(self):\n",
    "        try:\n",
    "            images, targets = next(self.dataiter)\n",
    "        except StopIteration:\n",
    "            self.dataiter = iter(self.dataloader)\n",
    "            images, targets = next(self.dataiter)\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = network(images)\n",
    "        losses = [c(network, logits, targets) for c in closures]\n",
    "        \n",
    "        # Get jacobian with respect to each loss.\n",
    "        # `allow_unused=True` to get gradient from the unused tail.\n",
    "        #     It returns `None`, which will be filtered later.\n",
    "        # `retain_graph=True` to retain forward information for\n",
    "        #     second-time backward.\n",
    "        # `create_graph=True` to create computation graph for\n",
    "        #     second-order derivation.\n",
    "        param_grads = [list(torch.autograd.grad(\n",
    "            l, network.parameters(), allow_unused=True,\n",
    "            retain_graph=True, create_graph=True)) for l in losses]\n",
    "\n",
    "        # As metioned above, `allow_unused=True` leads to `None`s in\n",
    "        #     jacobian tuple. Now we replace it with a zero tensor.\n",
    "        for param_grad in param_grads:\n",
    "            for i, (param_grad_module, param) in enumerate(zip(param_grad, network.parameters())):\n",
    "                if param_grad_module is None:\n",
    "                    param_grad[i] = torch.zeros_like(param)\n",
    "                    \n",
    "        return torch.stack([parameters_to_vector(param_grad) for param_grad in param_grads], dim=0)\n",
    "\n",
    "    @contextmanager\n",
    "    def init(self, alpha):\n",
    "        try:\n",
    "            alpha = torch.as_tensor(alpha.astype(self.dtype), device=device).view(1, -1)\n",
    "            jacobians = self._get_jacobians()\n",
    "            self.alpha_jacobians = alpha.matmul(jacobians).squeeze()\n",
    "            yield self\n",
    "        finally:\n",
    "            self.alpha_jacobians = None\n",
    "\n",
    "    def _matvec_tensor(self, tensor):\n",
    "\n",
    "        # hvp = Hv\n",
    "        #     = dot(∂^2(f) / (∂x)^2, v)\n",
    "        #     = ∂/∂x(dot(v, ∂f/∂x))\n",
    "\n",
    "        # dot = dot(v, ∂f/∂x)\n",
    "        dot = self.alpha_jacobians.dot(tensor)\n",
    "        \n",
    "        # hvp = ∂/∂x(dot)\n",
    "        param_alphas_hvps = torch.autograd.grad(dot, network.parameters(), retain_graph=True)\n",
    "        alphas_hvps = parameters_to_vector([p.contiguous() for p in param_alphas_hvps])\n",
    "\n",
    "        if damping > 0.0:\n",
    "            alphas_hvps.add_(tensor, alpha=damping)\n",
    "        return alphas_hvps\n",
    "\n",
    "    def _matvec(self, x):\n",
    "        \"\"\"HVP matrix-vector multiplication handler.\n",
    "\n",
    "        If self is a linear operator of shape (N, N), then this method will\n",
    "        be called on a shape (N,) or (N, 1) ndarray, and should return a\n",
    "        shape (N,) or (N, 1) ndarray.\n",
    "\n",
    "        In our case, it computes alpha_hession @ x.\n",
    "        \"\"\"\n",
    "        tensor = torch.as_tensor(x.astype(self.dtype), device=device)\n",
    "        ret = self._matvec_tensor(tensor)\n",
    "        return ret.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions\n",
    "- assign parameter.grad from vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_grad(vector, normalize=True):\n",
    "    if normalize:\n",
    "        vector.div_(vector.norm())\n",
    "    offset = 0\n",
    "    for p in network.parameters():\n",
    "        numel = p.numel()\n",
    "        # view as to avoid deprecated pointwise semantics\n",
    "        p.grad = vector[offset:offset + numel].view_as(p.data).clone()\n",
    "        offset += numel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's explore it!\n",
    "Executing this section takes around 30 to 45 minutes depending on your GPU types. However, for the results reported in the paper and supplemental material, our training process is typically a lot faster than here because of the following reasons:\n",
    "\n",
    "- For reproducibility, we disable randomness in this script as much as we can. Therefore, parallalism on GPUs is not fully exploited;\n",
    "- We frequently evaluate and save the model inside the innermost loop, which creates a lot of overhead;\n",
    "- Since the MINRES implementation is from scipy and is on CPUs, calling `HVPLinearOperator` creates a lot of CPU-GPU communication. Ideally, a GPU implementation of MINRES could completely remove this communication and expedite the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_op_template = HVPLinearOperator(mr_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, weight in enumerate(tqdm(evenly_dist(num_weights, 3), desc='Weight', leave=False)):\n",
    "    for col in trange(6, desc='Direction', leave=False):\n",
    "        \n",
    "        mr_weights = np.array(\n",
    "            [int(w) for w in '{:0>3b}'.format(col + 1)]).astype(linear_op_template.dtype).reshape(1, -1)\n",
    "        \n",
    "        # load SGD starting point\n",
    "        init_ckpt = torch.load(sgd_path / '{:d}.pth'.format(i), map_location='cpu')\n",
    "        network.load_state_dict(init_ckpt['state_dict'])\n",
    "\n",
    "        # initalize momentum buffer\n",
    "        jacobians_buffer_tensor = compute_jacobians()\n",
    "        jacobians_buffer = jacobians_buffer_tensor.clone().detach().cpu().numpy()\n",
    "        alpha_buffer = compute_alpha(jacobians_buffer_tensor)\n",
    "        with trange(num_steps, desc='Step', leave=False) as step_iter:\n",
    "            for step in step_iter:\n",
    "                network.train(False)\n",
    "\n",
    "                # compute jacobians\n",
    "                jacobians_tensor = compute_jacobians(1.0 / 4.0)\n",
    "                jacobians = jacobians_tensor.clone().detach().cpu().numpy()\n",
    "                jacobians_buffer *= momentum\n",
    "                jacobians_buffer += (1 - momentum) * jacobians\n",
    "                jacobians = jacobians_buffer.copy()\n",
    "\n",
    "                # compute alpha\n",
    "                alpha = compute_alpha(jacobians_tensor)\n",
    "                alpha_buffer *= momentum\n",
    "                alpha_buffer += (1 - momentum) * alpha\n",
    "                alpha = alpha_buffer.copy()\n",
    "\n",
    "                # define rhs and x0\n",
    "                rhs = np.squeeze(mr_weights @ jacobians)\n",
    "                x0 = jacobians.mean(axis=0)\n",
    "                \n",
    "                # fill jacobians alpha rhs x0 to MINRES\n",
    "                with linear_op_template.init(alpha) as linear_op:\n",
    "                    results = minres(linear_op, rhs, x0=x0, maxiter=maxiter)\n",
    "                    d = torch.as_tensor(results[0].astype(linear_op.dtype), device=device)\n",
    "\n",
    "                # optimize\n",
    "                mr_optimizer.zero_grad()\n",
    "                assign_grad(d, normalize=True)\n",
    "                mr_optimizer.step()\n",
    "\n",
    "                eval_losses, eval_top1s = evaluate(network, testloader, closures, top1_closures)\n",
    "                step_iter.set_postfix(**{'acc-{:d}'.format(i + 1): top for i, top in enumerate(eval_top1s)})\n",
    "                ckpt = {\n",
    "                    'state_dict': network.state_dict(),\n",
    "                    'optimizer': mr_optimizer.state_dict(),\n",
    "                    'metrics': [eval_losses, eval_top1s]\n",
    "                }\n",
    "                save_path = mr_path / str(i) / str(col)\n",
    "                save_path.mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(ckpt, save_path / '{:d}.pth'.format(step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw=dict(projection='3d', proj_type='ortho'))\n",
    "cmap = plt.get_cmap('autumn', len(evenly_dist(num_weights, 3)))\n",
    "\n",
    "total_top1s = []\n",
    "for i, weight in enumerate(evenly_dist(num_weights, 3)):\n",
    "    ckpt = torch.load(sgd_path / '{:d}.pth'.format(i), map_location='cpu')\n",
    "    losses, top1s = ckpt['metrics']\n",
    "    total_top1s.append(top1s)\n",
    "total_top1s = np.stack(total_top1s, axis=0).T\n",
    "top1s_min = total_top1s.min(axis=1) * 0.99\n",
    "total_err1s = 100.0 * (1.0 - total_top1s)\n",
    "ax.scatter(*total_err1s, color=[cmap(i) for i, w in enumerate(evenly_dist(num_weights, 3))],\n",
    "           marker='*', s=200, edgecolor='black')\n",
    "\n",
    "for i, weight in enumerate(evenly_dist(num_weights, 3)):\n",
    "    total_top1s = []\n",
    "    for col in range(6):\n",
    "        for step in range(num_steps):\n",
    "            ckpt = torch.load(mr_path / str(i) / str(col) /  '{:d}.pth'.format(step), map_location='cpu')\n",
    "            losses, top1s = ckpt['metrics']\n",
    "            if any(top1s < top1s_min):\n",
    "                continue\n",
    "            total_top1s.append(top1s)\n",
    "    total_err1s = 100.0 * (1.0 - np.stack(total_top1s, axis=0).T)\n",
    "    ax.scatter(*total_err1s, color=cmap(i), marker='o', s=30)\n",
    "\n",
    "ax.set_xlabel('Task 1 Top-1 Error')\n",
    "ax.set_ylabel('Task 2 Top-1 Error')\n",
    "ax.set_zlabel('Task 3 Top-1 Error')\n",
    "ax.grid(True)\n",
    "\n",
    "handles, labels = [], []\n",
    "handles.append(\n",
    "    tuple(ax.scatter([], [], [], color=cmap(i), marker='*', s=100, edgecolor='black') for i in [3, 6, 9]))\n",
    "labels.append('Start')\n",
    "\n",
    "handles.append(\n",
    "    tuple(ax.scatter([], [], [], color=cmap(i), marker='o', s=30) for i in [3, 6, 9]))\n",
    "labels.append('Ours')\n",
    "\n",
    "ax.legend(handles, labels, handler_map={tuple: HandlerTuple(None, 0)})\n",
    "ax.view_init(30, 45)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
