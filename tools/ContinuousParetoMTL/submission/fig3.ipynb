{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Continuous Pareto Exploration in Multi-Task Learning\n",
    "Source code for ICML submission #640 \"Efficient Continuous Pareto Exploration in Multi-Task Learning\"\n",
    "\n",
    "This script generates Figure 3 in the paper.\n",
    "\n",
    "# Problem setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import LinearOperator, minres\n",
    "import scipy.optimize\n",
    "\n",
    "from common import *\n",
    "from zdt2_variant import Zdt2Variant\n",
    "from pretty_tabular import PrettyTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define hyperparameters and random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2               # Number of tangent directions we want to generate.\n",
    "N = 10              # Number of points to be collected.\n",
    "s = 0.1             # Step size.\n",
    "maxiter = 2         # Maximum allowable number of iterations in MINRES.\n",
    "gamma = 0.9         # The decay factor in line search.\n",
    "num_exp = 10        # How many times you want to repeat the experiment with a different seed.\n",
    "# Hyperparameters for line search.\n",
    "ths = 1e-5          # For detecting convergence. Smaller value takes longer to converge.\n",
    "eta_init = 1        # Initial step size in line search.\n",
    "c1 = 0              # For strong Wolfe condition. Cannot be negative. Typically 1e-4. Larger values requires more iterations in line search.\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import the ZDT2-variant problem definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = Zdt2Variant()\n",
    "n, m = problem.n, problem.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our method\n",
    "We define a few functions here: a function to solve $\\alpha$ as described in Equation (3) of the paper, a function that does line search for MGDA, and our MGDA function. Specifically, `mgda_optimize` will be used to obtain a Pareto optimal solution from any initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve alpha in MGDA: note that this does not increment the counter and g[0] and g[1] are not necessarily parallel.\n",
    "def compute_alpha(g):\n",
    "    g = ndarray(g)\n",
    "    assert g.shape == (m, n)\n",
    "    alpha = cp.Variable(m)\n",
    "    objective = cp.Minimize(cp.sum_squares(alpha.T @ g))\n",
    "    constraints = [alpha >= 0, cp.sum(alpha) == 1]\n",
    "    alpha_prob = cp.Problem(objective, constraints)\n",
    "    optimal_loss = alpha_prob.solve()\n",
    "    alpha = ndarray(alpha.value).ravel()\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line search algorithm. Note that this function increments the counter of #f calls.\n",
    "# x: current location. R^n.\n",
    "# f: f(x). R^m.\n",
    "# grad: grad(x). R^{mxn}\n",
    "# d: a *descent* direction for all f. R^n.\n",
    "# The goal is to find eta such that f(x + eta * d) <= f(x) + c1 * grad_f(x).dot(d) * eta for every f.\n",
    "def line_search(x, f, grad, d, eta, c1):\n",
    "    x = ndarray(x).ravel()\n",
    "    assert x.size == n\n",
    "    f = ndarray(f).ravel()\n",
    "    assert f.size == m\n",
    "    grad = ndarray(grad)\n",
    "    assert grad.shape == (m, n)\n",
    "    d = ndarray(d).ravel()\n",
    "    assert d.size == n\n",
    "    while True:\n",
    "        x_new = x + eta * d\n",
    "        f_new = problem.f(x_new)\n",
    "        if np.all([fi_new <= fi + c1 * gradi.dot(d) * eta for fi, gradi, fi_new in zip(f, grad, f_new)]):\n",
    "            return eta\n",
    "        eta *= gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto optimization: pushing an unoptimized x to the Pareto set.\n",
    "# x: current location. R^n.\n",
    "def mgda_optimize(x):\n",
    "    x = ndarray(x).ravel()\n",
    "    assert x.size == n\n",
    "    x_iter = np.copy(x)\n",
    "    while True:\n",
    "        g_iter = problem.grad(x_iter)\n",
    "        f_iter = problem.f(x_iter)\n",
    "        alpha_iter = compute_alpha(g_iter)\n",
    "        # Negative sign here because d must be a *descent* direction.\n",
    "        d = -ndarray(alpha_iter.T @ g_iter).ravel()\n",
    "        # Make sure they are indeed descent.\n",
    "        for gi in g_iter:\n",
    "            assert gi.dot(d) <= 0 or np.isclose(gi.dot(d), 0)\n",
    "        # Termination condition 1: gradient is too small.\n",
    "        if np.linalg.norm(d) < ths:\n",
    "            return x_iter\n",
    "        eta = line_search(x_iter, f_iter, g_iter, d, eta_init, c1)\n",
    "        x_iter += eta * d\n",
    "        # Termination condition 2: change is too little. Effectively, this means eta is too small.\n",
    "        if eta * np.linalg.norm(d) < ths:\n",
    "            return x_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our expansion method and a baseline method for comparison. The weighted sum baseline expands the local Pareto set as if it is executing the first iteration of SGD with a perturbed weight combination of two losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto expansion: compare two methods below.\n",
    "# Method 1 (baseline): perturbed alpha and combine two gradients (weighted_sum_expand).\n",
    "# Method 2 (MINRES): our method. Use MINRES to solve an approximated tangent after 2 iterations (minres_expand).\n",
    "# For both methods, we normalize the directions returned so that the step size s has the same effects.\n",
    "def weighted_sum_expand(x):\n",
    "    x = ndarray(x).ravel()\n",
    "    assert x.size == n\n",
    "    g = problem.grad(x)\n",
    "    alpha = compute_alpha(g)\n",
    "    # Perturb alpha a bit.\n",
    "    alpha_perturbed = alpha * np.random.uniform(0.9, 1.1, size=(K, m))\n",
    "    # Normalize each row.\n",
    "    alpha_perturbed /= np.sum(alpha_perturbed, axis=1)[:, None]\n",
    "    d = alpha_perturbed @ g\n",
    "    # Normalize d.\n",
    "    # If we do not normalize di, GD can walk around the Pareto front without too much correction but the step size\n",
    "    # is extremely small because the gradient direction is actually orthogonal to the Pareto set in ZDT2-variant.\n",
    "    d_norm = np.sqrt(np.sum(d ** 2, axis=1))[:, None]\n",
    "    d /= d_norm\n",
    "    # Minus sign as if we are running gradient-*descent* for one step.\n",
    "    return ndarray(x - s * d)\n",
    "\n",
    "def minres_expand(x):\n",
    "    x = ndarray(x).ravel()\n",
    "    assert x.size == n\n",
    "    g = problem.grad(x)\n",
    "    alpha = compute_alpha(g)\n",
    "\n",
    "    def H_op(y):\n",
    "        y = ndarray(y).ravel()\n",
    "        assert y.size == n\n",
    "        return problem.hvp(x, alpha, y)\n",
    "\n",
    "    # Generate K normalized directions.\n",
    "    vi = []\n",
    "    for _ in range(K):\n",
    "        b = np.random.normal(size=m).T @ g\n",
    "        x_sol, _ = minres(LinearOperator((n, n), matvec=H_op, rmatvec=H_op), b, maxiter=maxiter)\n",
    "        x_sol = ndarray(x_sol)\n",
    "        x_sol /= np.linalg.norm(x_sol)\n",
    "        vi.append(x_sol)\n",
    "    vi = ndarray(vi)\n",
    "    return ndarray(x + s * vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "We now generate Figure 3 and print a table that summarizes the time cost of our method and the baseline. Note that our method spends slightly more Hessian-vector-product calls when determining the expansion directions in exchange for much more efficient optimization afterwards. In particular, solutions (orange circles) explored by our method closely track the analytic Pareto front. In the meantime, the baseline uses a cheap direction to explore new solutions but costs much more to optimize them back to the Pareto front. You are welcome to tune the step size or hyperparameters of line search to improve the baseline. Hopefully, the advantage of our method over this baseline should be observed consistently.\n",
    "\n",
    "If you cannot see figures, try replacing `%matploatlib tk` with `%matplotlib inline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out '%matplotlib tk' and uncomment '%matplotlib inline' if this cell does not generate figures on you computer.\n",
    "%matplotlib tk\n",
    "#%matplotlib inline\n",
    "\n",
    "# Simplifying the counter.\n",
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.f_cnt = 0\n",
    "        self.g_cnt = 0\n",
    "        self.h_cnt = 0\n",
    "\n",
    "    def add(self, f_cnt, g_cnt, h_cnt):\n",
    "        self.f_cnt += f_cnt\n",
    "        self.g_cnt += g_cnt\n",
    "        self.h_cnt += h_cnt\n",
    "\n",
    "# Set up the figure.\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "for idx, (name, expand) in enumerate(zip(('MINRES', 'MGDA'), (minres_expand, weighted_sum_expand))):\n",
    "    # Repeat the same experiments with 10 random seeds.\n",
    "    pareto_fronts = []\n",
    "    explored = []\n",
    "\n",
    "    # Counter starts here.\n",
    "    expand_counter = Counter()\n",
    "    optimize_counter = Counter()\n",
    "    head = { 'method ({})'.format(name): '{:>20}', 'eval_f_cnt': '{:4d}', 'eval_g_cnt': '{:4d}', 'eval_hvp_cnt': '{:4d}' }\n",
    "    tabular = PrettyTabular(head)\n",
    "    print_info(tabular.head_string())\n",
    "    for seed in range(num_exp):\n",
    "        np.random.seed(seed)\n",
    "        # Generate the initial Pareto optimal point.\n",
    "        x0 = problem.sample_pareto_set()\n",
    "        f0 = problem.f(x0)\n",
    "        g0 = problem.grad(x0)\n",
    "        alpha0 = compute_alpha(g0)\n",
    "        # BFS starts here.\n",
    "        q = deque()\n",
    "        q.append((x0, f0))\n",
    "        pareto_front = [f0]\n",
    "\n",
    "        while q:\n",
    "            xi, fi = q.popleft()\n",
    "            # Expand.\n",
    "            problem.reset_count()\n",
    "            x1 = expand(xi)\n",
    "            # Update counter.\n",
    "            expand_counter.add(problem.eval_f_cnt, problem.eval_grad_cnt, problem.eval_hvp_cnt)\n",
    "    \n",
    "            f1 = ndarray([problem.f(x1i) for x1i in x1])\n",
    "            explored.append(f1)\n",
    "\n",
    "            # Optimize.\n",
    "            problem.reset_count()\n",
    "            x2 = [mgda_optimize(x1i) for x1i in x1]\n",
    "            # Update counter.\n",
    "            optimize_counter.add(problem.eval_f_cnt, problem.eval_grad_cnt, problem.eval_hvp_cnt)\n",
    "\n",
    "            f2 = ndarray([problem.f(x2i) for x2i in x2])\n",
    "            # Add back to the queue.\n",
    "            for x2i in x2:\n",
    "                f2i = problem.f(x2i)\n",
    "                q.append((x2i, f2i))\n",
    "                pareto_front.append(f2i)\n",
    "            problem.reset_count()\n",
    "\n",
    "            # Terminate if we reach the limit.\n",
    "            if len(pareto_front) > N:\n",
    "                break\n",
    "        pareto_fronts.append(pareto_front)\n",
    "    # Print the counter information.\n",
    "    row_data = { 'method ({})'.format(name): 'expand', 'eval_f_cnt': expand_counter.f_cnt,\n",
    "                 'eval_g_cnt': expand_counter.g_cnt, 'eval_hvp_cnt': expand_counter.h_cnt }\n",
    "    print(tabular.row_string(row_data))\n",
    "    row_data = { 'method ({})'.format(name): 'optimize', 'eval_f_cnt': optimize_counter.f_cnt,\n",
    "                 'eval_g_cnt': optimize_counter.g_cnt, 'eval_hvp_cnt': optimize_counter.h_cnt }\n",
    "    print(tabular.row_string(row_data))\n",
    "\n",
    "    pareto_fronts = ndarray(np.vstack(pareto_fronts))\n",
    "    explored = ndarray(np.vstack(explored))\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, idx + 1)\n",
    "    # Plot the Pareto front.\n",
    "    problem.plot_pareto_front(ax, label='Pareto front')\n",
    "    ax.scatter(explored[:, 0], explored[:, 1], c='tab:orange', s=45, alpha=0.7, label='$x_i$')\n",
    "    ax.scatter(pareto_fronts[:, 0], pareto_fronts[:, 1], c='tab:red', s=15, label='$f(x^*_i)$')\n",
    "    ax.legend()\n",
    "    ax.set_xlim([-0.1, 1.1])\n",
    "    ax.set_ylim([-0.1, 1.4])\n",
    "    ax.set_xticks(np.linspace(0, 1, 6))\n",
    "    ax.set_yticks(np.linspace(0, 1.2, 7))\n",
    "    ax.set_title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
